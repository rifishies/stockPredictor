# -*- coding: utf-8 -*-
"""stockPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sQzrWJ_sbo09k_eLGpdGdCMF5rtCi-JE
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt

# Dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))

        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])
        return out

# Data preparation function
def prepare_data(data, look_back=60):
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data.reshape(-1, 1))

    X, y = [], []
    for i in range(len(scaled_data) - look_back):
        X.append(scaled_data[i:(i + look_back)])
        y.append(scaled_data[i + look_back])

    return np.array(X), np.array(y), scaler

# Training function
def train_model(model, train_dataloader, criterion, optimizer, device, epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for x_batch, y_batch in train_dataloader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)

            outputs = model(x_batch)
            loss = criterion(outputs, y_batch.unsqueeze(1))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataloader)
        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')

# Main execution
if __name__ == "__main__":
    # Hyperparameters
    look_back = 60
    input_size = 1
    hidden_size = 50
    num_layers = 2
    output_size = 1
    learning_rate = 0.001
    batch_size = 16
    epochs = 10

    # Generate sample data
    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')
    data = np.sin(np.linspace(0, 100, len(dates))) + np.random.normal(0, 0.1, len(dates))
    data_filtered_ext = pd.DataFrame({'Close': data}, index=dates)

    # Prepare data
    train_data_len = int(len(data) * 0.8)
    train_data = data[:train_data_len]
    test_data = data[train_data_len-look_back:]

    # Create sequences
    x_train, y_train, scaler_train = prepare_data(train_data, look_back)
    x_test, y_test, scaler_test = prepare_data(test_data, look_back)

    # Setup device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Create model
    model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)
    criterion = nn.L1Loss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Convert to Torch tensors
    x_train = torch.from_numpy(x_train).float().to(device)
    y_train = torch.from_numpy(y_train).float().to(device)
    x_test = torch.from_numpy(x_test).float().to(device)
    y_test = torch.from_numpy(y_test).float().to(device)

    # Create data loaders
    train_dataset = TimeSeriesDataset(x_train, y_train)
    test_dataset = TimeSeriesDataset(x_test, y_test)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Train model
    train_model(model, train_dataloader, criterion, optimizer, device, epochs)

    # Evaluation
    model.eval()
    with torch.no_grad():
        y_pred_scaled = model(x_test)
        y_pred_scaled = y_pred_scaled.cpu()
        y_pred_scaled_numpy = y_pred_scaled.numpy()
        y_test_numpy = y_test.cpu().numpy()

    # Unscale predictions
    y_pred = scaler_test.inverse_transform(y_pred_scaled_numpy)
    y_test_unscaled = scaler_test.inverse_transform(y_test_numpy.reshape(-1, 1))

    # Calculate metrics
    MAE = mean_absolute_error(y_test_unscaled, y_pred)
    MAPE = np.mean(np.abs((y_test_unscaled - y_pred) / y_test_unscaled)) * 100
    accuracy = 100 - MAPE

    print(f'Mean Absolute Error (MAE): {np.round(MAE, 2)}')
    print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)}%')
    print(f'Accuracy: {np.round(accuracy, 2)}%')

    # Plotting results
    plt.figure(figsize=(15, 6))
    plt.plot(y_test_unscaled, label='Actual')
    plt.plot(y_pred, label='Predicted')
    plt.title('Time Series Prediction')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.show()

    # Plot metrics
    metrics = {
        'MAE': MAE,
        'MAPE': MAPE,
        'Accuracy': accuracy
    }

    plt.figure(figsize=(10, 6))
    plt.bar(metrics.keys(), metrics.values())
    plt.title('Model Performance Metrics')
    plt.ylabel('Value')
    plt.show()